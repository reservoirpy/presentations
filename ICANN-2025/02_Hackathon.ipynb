{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a7d3e3",
   "metadata": {},
   "source": [
    "# ICANN 2025 Hackathon\n",
    "\n",
    "Now that you have followed the presentation (see [its notebook here](./01_Introduction.ipynb)), it's time for you to play with the library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc92008",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet reservoirpy[hyper]==0.4.* dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2869dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from reservoirpy import ESN\n",
    "from reservoirpy.nodes import Reservoir, Ridge, ES2N, IPReservoir, NVAR\n",
    "from reservoirpy.observables import nrmse, rsquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9c3d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset definition\n",
    "from reservoirpy.datasets import lorenz\n",
    "timeseries = lorenz(1020)[20:]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(timeseries)\n",
    "plt.title(\"Lorenz timeseries\")\n",
    "plt.subplot(1, 2, 2, projection=\"3d\")\n",
    "plt.plot(timeseries[:,0], timeseries[:,1], timeseries[:,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8237c268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale\n",
    "mins = timeseries.min(axis=0)\n",
    "maxs = timeseries.max(axis=0)\n",
    "timeseries = (timeseries - mins) / (maxs - mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a973c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from it\n",
    "from reservoirpy.datasets import to_forecasting\n",
    "dataset = to_forecasting(timeseries, forecast=10, test_size=0.2)\n",
    "x_train, x_test, y_train, y_test = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d50eb",
   "metadata": {},
   "source": [
    "# Play around with *ReservoirPy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb8e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your model here\n",
    "\n",
    "# You can play with the different nodes: \n",
    "# IPReservoir (intrinsic plasticity), NVAR (next-generation RC), LocalPlasticityReservoir (synaptic plasticity), ...\n",
    "# You can also compose nodes together to create a more complex model (DeepESN, hierarchical ESN, ...)\n",
    "model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ba7158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and run\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.run(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb0f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate predictions\n",
    "print(f\"NRMSE = {nrmse(y_test, y_pred, dimensionwise=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cf83b5",
   "metadata": {},
   "source": [
    "# Optimize hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2d52e6",
   "metadata": {},
   "source": [
    "## **Step 1**: Define the objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc08f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset to use:\n",
    "dataset = x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d40fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective functions accepted by ReservoirPy must respect some conventions:\n",
    "#  - dataset and config arguments are mandatory, like the empty '*' expression.\n",
    "#  - all parameters that will be used during the search must be placed after the *.\n",
    "#  - the function must return a dict with at least a 'loss' key containing the result of the loss function.\n",
    "# You can add any additional metrics or information with other keys in the dict. See hyperopt documentation for more informations.\n",
    "def objective(dataset, config, *, input_scaling, N, sr, lr, ridge, seed):\n",
    "    # This step may vary depending on what you put inside 'dataset'\n",
    "    x_train, x_test, y_train, y_test = dataset\n",
    "\n",
    "    # You can access anything you put in the config file from the 'config' parameter.\n",
    "    instances = config[\"instances_per_trial\"]\n",
    "\n",
    "    # The seed should be changed across the instances to be sure there is no bias in the results due to initialization.\n",
    "    variable_seed = seed\n",
    "\n",
    "    losses = []; r2s = [];\n",
    "    for n in range(instances):\n",
    "        # Build your model given the input parameters\n",
    "        model = ESN(units=200)\n",
    "        # raise NotImplementedError(\"Remove this error and put your model here!\")\n",
    "\n",
    "        # Train your model and test your model.\n",
    "        predictions = model.fit(x_train, y_train) \\\n",
    "                           .run(x_test)\n",
    "\n",
    "        loss = nrmse(y_test, predictions, norm_value=np.ptp(x_train))\n",
    "        r2 = rsquare(y_test, predictions)\n",
    "\n",
    "        # Change the seed between instances\n",
    "        variable_seed += 1\n",
    "\n",
    "        losses.append(loss)\n",
    "        r2s.append(r2)\n",
    "\n",
    "    # Return a dictionnary of metrics. The 'loss' key is mandatory when using hyperopt.\n",
    "    return {'loss': np.mean(losses),\n",
    "            'r2': np.mean(r2s)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8028db",
   "metadata": {},
   "source": [
    "## **Step 2**: Define the research space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef0cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "hyperopt_config = {\n",
    "    \"exp\": \"hyperopt-1\",    # the experimentation name\n",
    "    \"hp_max_evals\": 200,              # the number of differents sets of parameters hyperopt has to try\n",
    "    \"hp_method\": \"random\",            # the method used by hyperopt to chose those sets (see below)\n",
    "    \"seed\": 42,                       # the random state seed, to ensure reproducibility\n",
    "    \"instances_per_trial\": 1,         # how many random ESN will be tried with each sets of parameters\n",
    "    \"hp_space\": {                     # what are the ranges of parameters explored\n",
    "        \"N\": [\"choice\", 100],             # the number of neurons is fixed to 100\n",
    "        \"sr\": [\"loguniform\", 1e-5, 1e2],   # the spectral radius is log-uniformly distributed from 1e-5 and 100\n",
    "        \"lr\": [\"loguniform\", 1e-5, 1.0],    # idem with the leaking rate, from 1e-5 to 1\n",
    "        \"input_scaling\": [\"choice\", 1.0],   # the input scaling is fixed\n",
    "        \"ridge\": [\"loguniform\", 1e-6, 1e6],        # regularization parameter is explored widely\n",
    "        \"seed\": [\"randint\", 0, 2**32]          # seeds used for random initialisation of weights matrices\n",
    "    }\n",
    "}\n",
    "\n",
    "# we precautionously save the configuration in a JSON file\n",
    "# each file will begin with a number corresponding to the current experimentation run number.\n",
    "with open(f\"{hyperopt_config['exp']}.config.json\", \"w+\") as f:\n",
    "    json.dump(hyperopt_config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce6dfc",
   "metadata": {},
   "source": [
    "## **Step 3**: Launch the hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd178ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reservoirpy.hyper import parallel_research\n",
    "best = parallel_research(objective, dataset, f\"{hyperopt_config['exp']}.config.json\", \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf05490c",
   "metadata": {},
   "source": [
    "## **Step 4**: Plot report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af3fdc5",
   "metadata": {},
   "source": [
    "#### Static method using *ReservoirPy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0aff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reservoirpy.hyper import plot_hyperopt_report\n",
    "fig = plot_hyperopt_report(hyperopt_config[\"exp\"], (\"lr\", \"sr\", \"ridge\"), metric=\"r2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfea45d",
   "metadata": {},
   "source": [
    "#### Interactive report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bedc303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactive_report import interactive_report\n",
    "interactive_report(hyperopt_config['exp'], ['lr', 'sr', 'ridge'], log_loss=True, out_path='limit.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f626b7b",
   "metadata": {},
   "source": [
    "## **Step 5**: Refine the search space\n",
    "\n",
    "Let's refine the search space based on the new ranges we have found for the hyperparameters.\n",
    "And let's now allow the input scaling to vary to potentially find better optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b60fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "hyperopt_config = {\n",
    "    \"exp\": \"hyperopt-doublescroll-refined-01\",    # the experimentation name changed (so we can save the results in another folder)\n",
    "    \"hp_max_evals\": 200,              \n",
    "    \"hp_method\": \"random\",           \n",
    "    \"seed\": 42,                    \n",
    "    \"instances_per_trial\": 1,       \n",
    "    \"hp_space\": {                   \n",
    "        \"N\": [\"choice\", 100],            \n",
    "        \"sr\": [\"loguniform\", 1e-5, 1e2],    # we keep the SR ranges unchanged because the ranges seem already good and don't change much the loss\n",
    "        \"lr\": [\"loguniform\", 10**(-2.34), 1.0],    # change to new find range values \n",
    "        \"input_scaling\": [\"loguniform\", 1e-5, 1e2],    # making the input scaling variable: we take the same values as SR\n",
    "        \"ridge\": [\"loguniform\", 1e-10, 10**(-0.32)],       # we reduced the range on the right-hand part and augment it on the left-hand part\n",
    "        \"seed\": [\"randint\", 0, 2**32]          # an other random seed for the ESN initialization\n",
    "    }\n",
    "}\n",
    "\n",
    "# we precautionously save the configuration in a JSON file\n",
    "# each file will begin with a number corresponding to the current experimentation run number.\n",
    "with open(f\"{hyperopt_config['exp']}.config.json\", \"w+\") as f:\n",
    "    json.dump(hyperopt_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b43c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = parallel_research(objective, dataset, f\"{hyperopt_config['exp']}.config.json\", \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d718957",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_hyperopt_report(hyperopt_config[\"exp\"], (\"lr\", \"sr\", \"ridge\", \"input_scaling\"), metric=\"r2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
